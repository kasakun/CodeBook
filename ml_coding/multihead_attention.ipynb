{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+Q8mPyPgZko+qVUpkt87C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasakun/CodeBook/blob/master/ml_coding/multihead_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y4tK8VDbvxGt"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, num_heads, embedding_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "\n",
        "        self.W_Q = torch.nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.W_K = torch.nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.W_V = torch.nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "\n",
        "    def scaled_dot_production(self, query, key, value, mask=None):\n",
        "        # input query shape:  [batch_size, num_heads, seq_lenth, head_dim]\n",
        "        # scores shape : [batch_size, num_heads, seq_len, seq,len]\n",
        "        scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "        scores = scores / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # shape [batch_size, num_heads, seq_len, seq_len]\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # shape [batch_size, num_heads, seq_len, head_dim]\n",
        "        return torch.matmul(attention, value)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # query -> [batch_size, seq_len, embedding_size]\n",
        "        query = self.W_Q(query)\n",
        "        key = self.W_K(key)\n",
        "        value = self.W_V(value)\n",
        "\n",
        "        batch_size, seq_len, embedding_dim = query.shape\n",
        "\n",
        "        # Convert to shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(-1, -2)\n",
        "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(-1, -2)\n",
        "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(-1, -2)\n",
        "\n",
        "\n",
        "        attention = self.scaled_dot_production(query, key, value)\n",
        "\n",
        "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, embedding_dim)\n",
        "        # [batch_size, seq_len, embded_dim]\n",
        "        return attention"
      ],
      "metadata": {
        "id": "Wngls9_lvwwU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 8     # Embedding dimension (size of token vectors)\n",
        "num_heads = 2     # Number of attention heads\n",
        "seq_len = 5       # Length of the input sequence\n",
        "batch_size = 3    # Number of sequences in the batch\n",
        "\n",
        "# Create random input tensors for Q, K, and V\n",
        "query = torch.rand(batch_size, seq_len, embed_dim)  # (batch_size, seq_len, embed_dim)\n",
        "key = torch.rand(batch_size, seq_len, embed_dim)    # (batch_size, seq_len, embed_dim)\n",
        "value = torch.rand(batch_size, seq_len, embed_dim)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Instantiate the multi-head attention layer\n",
        "multihead_attn = MultiHeadAttention(num_heads, embed_dim)\n",
        "\n",
        "# Forward pass through the multi-head attention layer\n",
        "output = multihead_attn(query, key, value)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFeTVXeF9NiX",
        "outputId": "a143573d-d2cf-4fb8-a530-94fe87ec6cbf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5, 8])\n"
          ]
        }
      ]
    }
  ]
}